---
title: "An introduction to species distribution modelling using {tidysdm} & {tidymodels}"
description: |
  Species distribution modelling is a common task for ecologists in R. Here we show the fundamental steps to build, assess and use models to predict species distributions using {tidymodels} & {tidysdm}, modern packages that use tidy syntax to run and plot geospatial models.
author:
  - name: "Dax Kellie"
  - name: "Shandiya Balasubramaniam"
date: "2024-04-30"
title-block-banner: "#B8573E"
toc: true
toc-location: left
toc-depth: 2
categories:
  - Eukaryota
  - Animalia
  - Aves
  - Summaries
  - Maps
  - R
image: sdm-map.png
freeze: true
draft: false
editor_options: 
  chunk_output_type: console
---

```{r}
#| include: false
library(htmltools)
```

<!-- remove metadata section -->

```{=html}
<style>
  #title-block-header.quarto-title-block.default .quarto-title-meta {
      display: none;
  }
</style>
```
<!-- Author card -->

::: author-card
::: author-card-text
#### Author

[Dax Kellie](https://labs.ala.org.au/about/Kellie_Dax/)\
[Shandiya Balasubramaniam](https://labs.ala.org.au/about/Balasubramaniam_Shandiya/)

#### Date

30 April 2024
:::

::: author-card-image
```{r, out.width='120px', out.extra='style="clip-path: circle();"', echo=FALSE}
knitr::include_graphics("https://raw.githubusercontent.com/AtlasOfLivingAustralia/ala-labs/main/images/people/dax.jpg")
```
:::

::: author-card-image
```{r, out.width='120px', out.extra='style="clip-path: circle();"', echo=FALSE}
knitr::include_graphics("https://raw.githubusercontent.com/AtlasOfLivingAustralia/ala-labs/main/images/people/shandiya.png")
```
:::
:::

<!------------------------ Post starts here ------------------------>

Species distribution models (SDMs) offer a way to quantify relationships between biodiversity observations and environmental variables, and are widely used in biogeography, conservation biology, and macroecology. SDMs allow us to assess how suitable an area is for a species, which has implications for predicting range shifts of invasive or threatened species, and understanding habitat suitability under different climate change scenarios.

The process of fitting and choosing the best model is iterative and, often, different modelling approaches are used to make multiple predictions that can be averaged to get a final result. Here, we walk through the steps of building a SDM for the laughing kookaburra in a small area of New South Wales, Australia, using [tidymodels](https://www.tidymodels.org/) and [tidysdm](https://evolecolgroup.github.io/tidysdm/). tidymodels is a collection of packages for modelling using [tidyverse principles](https://tidyverse.tidyverse.org/articles/manifesto.html), and tidysdm implements SDMs using the tidymodels framework.


::: {.callout-note collapse="true"}
## Pros & cons of tidymodels

:::{layout-ncol="2"}

#### Pros

-   **Machine learning models are easier to build and use** <br>
[Tidymodels packages are good for machine learning models in R. Many types of SDMs like MaxEnt models are well-suited for tidymodels.]{style="font-size:.7rem;"}
-   **Data transformations are more transparent** <br>
[Steps like log-transformations, scaling and centring are occur *within* the model building workflow (rather than before during data wrangling), making these data manipulations more transparent to the model's results.]{style="font-size:.7rem;"}
-   **Functions are modular** <br>
[Tidymodels functions are modular, so many different types of models can be run with the same functions and workflow.]{style="font-size:.7rem;"}
-   **Many different models can be one at once** <br>
[Many different types of models can be run at once using a `workflow_set()`. These different models can also be compared and even combined into a final predictive model.]{style="font-size:.7rem;"}
-   **Auto-detects visualisations** <br>
[Helper functions like `autoplot()` make visualising model performance less cumbersome by determining which visual to display based on the previous object's output.]{style="font-size:.7rem;"}
-   **Make something pretty good quickly** <br>
[Users can build fairly good predictive models quite quickly, streamlining the process of going from raw data to prediction.]{style="font-size:.7rem;"}

#### Cons

-   **Learning the workflow can be hard** <br> 
[Learning tidymodels can be confusing and time consuming because the workflow differs quite a lot from other popular statistical modelling packages in R.]{style="font-size:.7rem;"}
-   **Confusing function names** <br>
[The {recipes} package uses function names like `prep()`, `bake()` and `juice()` which makes them difficult to know what they do or when to use them.]{style="font-size:.7rem;"}
-   **Learning function outputs takes time** <br> 
[Learning what output to expect from each function, and what helper functions are available for each step takes time.]{style="font-size:.7rem;"}
-   **Tidymodels isn't good at everything** <br>
[Tidymodels is optimised for machine learning, but it isn't necessarily "better" than other packages like {dismo}, {lme4}, {biomod2}, or packages for bayesian models like {tidybayes} or {brms}.]{style="font-size:.7rem;"}

:::

:::

To begin, we can load some packages.

```{r}
#| warning: false
#| message: false
library(galah)
library(tidyverse)
library(tidymodels) 
library(tidysdm) # devtools::install_github("EvolEcolGroup/tidysdm")
library(terra)
library(tidyterra)
library(here)
library(sf)
library(ozmaps)
```

# Download data

### Download biological data

Our model will use occurrence data of laughing kookaburras (*Dacelo novaeguineae*) in a small area in New South Wales. The laughing kookaburra is the largest Kingfisher in the world.

::: {layout-ncol="3" style="margin-left: auto; margin-right: auto;"}
<img src="https://ala-images.s3.ap-southeast-2.amazonaws.com/store/7/5/e/4/8987a472-fcbf-4ae5-be44-e5dc50eb4e57/original" class="rounded"/></img>

<img src="https://ala-images.s3.ap-southeast-2.amazonaws.com/store/3/c/6/0/27780a83-c4b3-4f44-ade3-2f401ac006c3/original" class="rounded"/></img>

<img src="https://ala-images.s3.ap-southeast-2.amazonaws.com/store/2/5/0/0/5c796423-708e-4e1c-adf6-72131ac40052/original" class="rounded"/></img>
:::

::: figure-caption
Left: [*Dacelo (Dacelo) novaeguineae* (craigc_86 CC-BY-NC 4.0 (Int))](https://biocache.ala.org.au/occurrences/0dccaf64-9aed-4b5e-bbc9-06a7ee3ae11e), Middle: [*Dacelo (Dacelo) novaeguineae* (Wildash \| questagame.com CC-BY-NC 4.0 (Int))](https://biocache.ala.org.au/occurrences/3e26b79d-803c-4b0a-95b7-05a39dad4caf), Right: [*Dacelo (Dacelo) novaeguineae* (c_a_critter CC-BY-NC 4.0 (Int))](https://biocache.ala.org.au/occurrences/6028fd90-df9b-4d12-86f8-08f47f005e61)
:::

::: aside
The laughing kookaburra also has one of the most recognisable bird calls. [This video is an amazing example](https://www.youtube.com/watch?v=TqdRQxgtZtI).
:::

We'll choose a region in New South Wales...

```{r}
# define geographic region
custom_bbox <- tibble(ymin = -35, 
                      ymax = -32, 
                      xmin = 149, 
                      xmax = 152.1)
```

::: aside
```{r}
#| echo: false
ggplot() +
  geom_sf(data = ozmaps::ozmap_states,
          fill = "white",
          colour = "grey30") + 
  geom_rect(data = custom_bbox,
            aes(xmin = xmin, 
                ymin = ymin, 
                xmax = xmax, 
                ymax = ymax),
            colour = "#E76F51",
            linewidth = 1.3,
            fill = NA) + 
  theme_light()
```
:::

...and download records from the ALA.

```{r}
#| eval: false
galah_config(email = "your-email-here") # Registered ALA email

# download occurrence records
kookaburras <- galah_call() |>
  identify("Dacelo novaeguineae") |>
  filter(year == 2023) |>
  galah_apply_profile(ALA) |>
  galah_geolocate(custom_bbox, type = "bbox") |>
  atlas_occurrences()
```

```{r}
#| echo: false
#| message: false
#| warning: false
galah_config(email = "dax.kellie@csiro.au", 
             verbose = FALSE)

# download occurrence records
kookaburras <- galah_call() |>
  identify("Dacelo novaeguineae") |>
  filter(year == 2023) |>
  galah_apply_profile(ALA) |>
  galah_geolocate(custom_bbox, type = "bbox") |>
  atlas_occurrences()
```

::: aside
```{r}
#| echo: false
ggplot() +
  geom_sf(data = ozmaps::ozmap_states,
          fill = "white",
          colour = "grey30") +
  geom_rect(data = custom_bbox,
            aes(xmin = xmin, 
                ymin = ymin, 
                xmax = xmax, 
                ymax = ymax),
            colour = "#E76F51",
            linewidth = 1.3,
            fill = NA) + 
  geom_point(data = kookaburras,
             aes(x = decimalLongitude, 
                 y = decimalLatitude),
             colour = "#264653",
             alpha = 0.3) + 
  theme_light() +
  theme(legend.position = "none")
```
:::

For many of the later steps we'll need the coordinates formatted as a spatial object (i.e., `geometry`). So, let's convert our occurrence data to a spatial object (`sf`) defined by the longitude and latitude coordinates, and set the Coordinate Reference System (CRS) to EPSG:4326[^crs] (WGS84).

[^crs]: ALA data is projected using [CRS EPSG:4326](https://epsg.io/4326) (the same one used by Google Earth).

```{r}
# convert to sf
kookaburras_sf <- kookaburras |>
  st_as_sf(coords = c("decimalLongitude", "decimalLatitude")) |>
  st_set_crs(4326)
```

### Download environmental data

To help with our model prediction, we'll also download the [BioClim variables](https://www.worldclim.org/data/bioclim.html), a list of 19 biologically relevant environmental variables, for all of Australia as a raster.

::: {.callout-note collapse="true"}
#### What's a raster?

A **raster** is a spatial grid of cells, where each cell contains a value representing information such as temperature or elevation. This information is often visualised by mapping colours to values in the raster (see image below). The resolution of the raster depends on the size of cells within the grid, with smaller cells corresponding to higher resolution (just like how the resolution of a television screen is determined by the number of pixels).

```{r}
#| code-fold: true
#| fig-cap: "The colour of the cell/pixel is determined by the value assigned to it"
plot_raster <- function(r) {
  plot(r, axes = FALSE, legend = FALSE)
  plot(as.polygons(r, dissolve = FALSE, trunc = FALSE), add = TRUE)
  text(r, digits = 2)
}

# Create a 4 x 4 matrix
m <- matrix(1:16, ncol = 4, nrow = 4)
# Convert the matrix into a raster
r16 <- rast(m)

plot_raster(r16)
```
:::

```{=html}
<!--
Note: This data exists in the Science & Decision Support folder on Microsoft Teams
* ./Data/science/projects/sdm-workflows/data
-->
```
```{r}
#| eval: false
# Download world climate data
bioclim <- geodata::worldclim_country(
    country = "Australia",
    var = "bio",
    res = 5,
    path = here::here("folder-name", 
                      "subfolder-name")
  )
```

```{r}
#| echo: false
# Load data from file:
bioclim <- terra::rast(here::here("posts",
                                  "data",
                                  "wc2.1_country",
                                  "AUS_wc2.1_30s_bio.tif"))
```

To narrow our BioClim data to only within the extent of our defined bounding box, we'll create an extent object `bbox_ext`, then crop our `bioclim` layers to within `bbox_ext` and project our cropped BioClim data to the same CRS as our kookaburra occurrence points.

```{r}
# Set the coordinates to our bounding box
bbox_ext <- terra::ext(
  c(custom_bbox[["xmin"]], 
    custom_bbox[["xmax"]], 
    custom_bbox[["ymin"]], 
    custom_bbox[["ymax"]]
    ))

# Crop our worldclim data to within our bounding box coordinates
aus <- bioclim |>
  terra::crop(bbox_ext) |>
  terra::project(crs("EPSG:4326"))
```

To make sure everything looks correct, let's plot one of the variables with `geom_spatraster()` from the [tidyterra package](https://dieghernan.github.io/tidyterra/)[^tidyterra].

[^tidyterra]: tidyterra follows the Grammar of Graphics made popular in R by ggplot2 and allows rasters to be plotted using the same syntax. In contrast, the terra package requires users to plot using base R styling (using `plot()`).

```{r}
# Download NSW map, set CRS projection
nsw <- ozmaps::ozmap_states |>
  filter(NAME == "New South Wales") |>
  st_transform(crs = st_crs(4326))

# Map of Annual temperature + points
first_map <- ggplot() +
  geom_spatraster(data = aus,
                  aes(fill = wc2.1_30s_bio_1)) +
  geom_sf(data = kookaburras_sf,
          colour = "#312108",
          size = 2) +
  scale_fill_whitebox_c(palette = "muted",
                        na.value = NA) +
  guides(fill = guide_colorbar(title = "Annual Mean\nTemperature")) +
  theme_void()

first_map
```


# Prepare data

Now that we have our occurrence data and environmental data, there are a few steps we'll complete to prepare our data for modelling.

### Thinning

The first step is to remove data where many points are overlapping. The resolution of our prediction is dependent on the resolution of our environmental data. If each cell in our environmental data defines the average value of one square kilometre, even if we had kookaburra observations at a higher resolution (for example, every square *metre* of our defined area), we can only detect differences to the lowest resolution of our grid cells.

So, we can **thin** our data so that there is only one observation per cell of our raster[^rast]. This step reduces spatial bias and lowers the risk that autocorrelation affects final predictions of our model.

[^rast]: It's also possible to thin data by distance rather than cell size using [`tidysdm::thin_by_dist()`](https://evolecolgroup.github.io/tidysdm/reference/thin_by_dist.html).

```{r}
# thin
set.seed(12345)
kookaburras_thin <- tidysdm::thin_by_cell(kookaburras_sf, 
                                          raster = aus
                                          )

# number of observations
tibble(
  before = nrow(kookaburras_sf),
  after = nrow(kookaburras_thin)
  )
```

::: aside
```{r}
#| echo: false
#| code-fold: true
#| code-summary: Code for map
# see results of thinning
ggplot() +
  geom_spatraster(data = aus,
                  aes(fill = wc2.1_30s_bio_1),
                  alpha = 0.1) +
  geom_rect(data = custom_bbox,
            mapping = aes(xmin = xmin, ymin = ymin, 
                          xmax = xmax, ymax = ymax),
            colour = "grey50",
            fill = NA) +
  geom_sf(data = kookaburras_thin,
          colour = "#312108",
          size = 2) +
  scale_fill_whitebox_c(palette = "muted",
                        na.value = NA) +
  theme_void() + 
  theme(legend.position = "none")
```
:::


### Pseudo-absences

Our data from the ALA is presence-only data. So, the second step is to create **pseudo-absences** (also called background points) that represent the full extent of the area where kookaburras haven't been observed (yet) in our data. Importantly, these are not the same as *true* absences and this should be taken into account when interpreting results[^trueabs].

[^trueabs]: A true absence has quite a different meaning than a pseudo-absence to a species distribution model. The main difference is in the value a known absence provides compared to a simulated one for our interpretation of the results.<br><br>A *true* absence is a point where, at a specific time, an organism was not found there. Alternatively, a pseudo-absence is a point that *acts* like we haven't found an animal there, but we don't actually have data for that location! The model, however, doesn't *know* if a point represents a true absence or a psuedo-absence. It only knows the information it is given and will interpret that information using the parameters it is provided (in this way, models are a reflection of the real-world, but never a substitute).<br><br>Collecting *true* absence data is difficult, typically requiring expert knowledge, surveys with stricter methodologies, and repeated measures of the same areas over time. Pseudo-absences are much easier to collect---you simply simulate them on a computer---but they are less informative. Keep this trade-off in mind as you interpret your model's results.

Let's add 3 times the number of presences to fill our grid, making sure they aren't closer than 5 km to another point like our occurrence points.

```{r}
kookaburras_pseudoabs <- tidysdm::sample_pseudoabs(
  kookaburras_thin,
  n = 3 * nrow(kookaburras_thin),
  raster = aus,
  method = c("dist_min", km2m(5))
)
```

::: aside
```{r}
#| echo: false
#| code-fold: true
#| code-summary: Code for map
# see pseudo absences
ggplot() +
  geom_spatraster(data = aus,
                  aes(fill = wc2.1_30s_bio_1),
                  alpha = 0.1) +
  geom_rect(data = custom_bbox,
            mapping = aes(xmin = xmin, 
                          ymin = ymin, 
                          xmax = xmax, 
                          ymax = ymax),
            colour = "grey50",
            fill = NA) +  
  geom_sf(data = kookaburras_pseudoabs,
          aes(col = class),
          size = 2) +
  scale_colour_manual(values = c("#312108", "#8A6A35")) +
  scale_fill_whitebox_c(palette = "muted",
                        na.value = NA) +
  guides(fill = "none") +
  theme_void()
```
:::

### Extract environmental values

Now that we have our presence and pseudo-absence points, the third step is to extract the environmental data for each point location in `aus` and bind the resulting values to our points data in `kookaburras_psuedoabs`. The result is a `tibble` with our points, their class, and the specific values of all 19 BioClim variables.

```{r}
kookaburras_bioclim <- kookaburras_pseudoabs |> 
  bind_cols(
    terra::extract(aus, 
                   kookaburras_pseudoabs, 
                   ID = FALSE)
    )
kookaburras_bioclim
```

### Select predictor variables

The fourth and final step is to choose predictor variables for our model. These are variables we think explain variation in our outcome (i.e., the probability a kookaburra could live in a given location). When choosing predictor variables, it is good practice to use theory and previous research to inform what variables you choose as predictors[^scale].

[^scale]: Keep in mind that the strength of variables depends on the scale of your prediction. If you wish to make predictions at a broad-scale, variables like temperature and rainfall will likely be strong predictors, whereas if you wish to make predictions at a fine-scale, variables like food scarcity and competition might be stronger predictors for your outcome.

:::{.callout-note collapse="true"}

#### Choosing variables and avoiding multicollinearity

In species distribution models, *multicollinearity*---high correlation between several independent variables in a model---can have unintended effects that bias predictions[^models]. Data science tools can also help refine your predictor variable choices, too, including [some functions in tidysdm](https://evolecolgroup.github.io/tidysdm/articles/a0_tidysdm_overview.html#thinning-step) that we used below.

[^models]: A model can only use the information it is provided to make inferences about the world. If multiple variables in a model correlate, the model can place too much weight on those values to determine the outcome! The model isn't aware of the many other environmental variables that affect the real world outcome.

A good start is to choose variables that differentiate between presences and pseudo-absences, which in the plot below are variables that have less overlap between red and blue distributions. To help choose variables with the highest non-overlapping distribution, we can decide on a percentage cut-off of 55% non-overlap, leaving us with the top 3 variables in the table below.

:::{.panel-tabset .nav-pills style="margin-bottom:15px;"}

## Plot

```{r}
#| code-fold: true
kookaburras_bioclim |>
  rename_with(
    ~ str_remove_all(.x, "wc2.1_30s_"), 
    starts_with("wc2.1_30s_")) |>
  plot_pres_vs_bg(class)
```

## Table

<style>
.output-scroll {
  max-height: 420px;
  overflow-y: scroll;
  margin-bottom: 20px;
}
</style>

```{r}
#| code-fold: true
#| class: output-scroll
overlap <- kookaburras_bioclim |>
  dist_pres_vs_bg(class) |>
  enframe("bioclim", "percent_non_overlap") |>
  arrange(desc(percent_non_overlap))

overlap |> gt::gt()
```

:::

Now we can use pair plots to view the relationship between each pair of variables. Variables `bio_12` and `bio_16` are very highly correlated (94%). To avoid multicollinearity in our model, we'll include only `bio_12` (annual precipitation) and `bio_4` (temperature seasonality).

```{r}
#| code-fold: true
aus |>
  select(wc2.1_30s_bio_12, 
         wc2.1_30s_bio_4, 
         wc2.1_30s_bio_16) |>
  terra::pairs()
```

:::

Here, we selected two BioClim variables that we thought were reasonable environmental predictors (using mainly data science techniques):

-   **BIO4**: Temperature Seasonality[^measure]
-   **BIO12**: Annual Precipitation

[^measure]: Measured as the standard deviation of the mean monthly temperature

We'll filter our point data and BioClim raster data to only include our two variables.

```{r}
# predictor variable names
vars <- c("wc2.1_30s_bio_4", "wc2.1_30s_bio_12")

# filter point data columns
kookaburras_bioclim_filtered <- 
  kookaburras_bioclim |> 
  select(all_of(c(vars, "class")))

kookaburras_bioclim_filtered |> head(5L)
```

```{r}
# filter bioclim data columns
aus_filtered <- aus[[vars]]
aus_filtered
```

```{r}
#| code-fold: true
bioclim4 <- ggplot() +
  geom_spatraster(data = aus_filtered,
                  aes(fill = wc2.1_30s_bio_4)) +
  geom_rect(data = custom_bbox,
            mapping = aes(xmin = xmin, 
                          ymin = ymin, 
                          xmax = xmax, 
                          ymax = ymax),
            colour = "grey50",
            fill = NA) +  
  scale_fill_whitebox_c(palette = "muted",
                        na.value = NA) +
  guides(fill = guide_colorbar(title = "Annual Range in\nTemperature\n(°C)")) +
  theme_void()

bioclim12 <- ggplot() +
  geom_spatraster(data = aus_filtered,
                  aes(fill = wc2.1_30s_bio_12)) +
  geom_rect(data = custom_bbox,
            mapping = aes(xmin = xmin, 
                          ymin = ymin, 
                          xmax = xmax, 
                          ymax = ymax),
            colour = "grey50",
            fill = NA) +  
  scale_fill_whitebox_c(palette = "deep",
                        na.value = NA) +
  guides(fill = guide_colorbar(title = "Precipitation (mm)")) +
  theme_void()
```

```{r}
#| echo: false
#| fig-column: body-outset
#| fig-align: center
#| layout-ncol: 2
#| layout-nrow: 1
#| fig-cap: 
#|   - "BioClim 4: Temperature Seasonality"
#|   - "BioClim 12: Annual Precipitation"
bioclim4
bioclim12
```

# Fit model

Tidymodels is designed to build a model workflow, train the model's performance, then test the model's ability to predict data accurately. This workflow might be slightly different to what many research scientists are used to.

In machine learning models, data is like a limited resource that we must divide using a "data budget" for two main purposes: training a reasonable model, and testing the final model.

### Split data

The first step to allocating your "data budget" is splitting your data. We can use `initial_split()` to allocate a reasonable "data budget" into these categories (typically a 75-25% split).

```{r}
# set training and testing data
set.seed(100)

kookaburras_split <- 
  kookaburras_bioclim_filtered |>
  initial_split()
kookaburras_split
```

Now we can save these data as separate data objects for `training()` and `testing()`.

```{r}
kookaburras_train <- training(kookaburras_split)
kookaburras_test <- testing(kookaburras_split)
```

We are left with two dataframes with identical columns but different points.

:::{.panel-tabset .nav-pills}

## Train

```{r}
kookaburras_train |> head(5L)
```

## Test

```{r}
kookaburras_test |> head(5L)
```

:::

### Resampling

Now let's resample our training data so we can use it to optimise and evaluate our model. One way to resample is using *cross-validation*, a well-established method of resampling that randomly assigns points to analysis and assessment groups. These randomly resampled and split data sets are known as *folds*. We can use the [spatialsample package](https://spatialsample.tidymodels.org/) to create 5 v-folds with `spatial_block_cv()`, a function for resampling spatial data.

```{r}
set.seed(100)
kookaburras_cv <- spatial_block_cv(kookaburras_train, v = 5)
```

`spatial_block_cv()` uses a type of resampling called *block cross-validation*, which creates a grid of "blocks" and attempts to maintain these blocked groups when resampling data points. Block cross-validation is important because spatial data is not completely random; data from neighbouring locations probably relate in some way (they aren't completely random), and block cross-validation attempts to preserve this spatial relationship. The plots below demonstrate the general process. The plot on the left shows the blocks, the animation on the right shows the resulting 5 folds.

:::{.column-page layout-ncol="2"}

:::{}

```{r}
#| fig-align: center
#| echo: false
autoplot(kookaburras_cv)
```

:::

:::{}

```{r splits-gif}
#| fig-align: center
#| animation-hook: gifski
#| echo: false
purrr::walk(kookaburras_cv$splits, function(x) print(autoplot(x)))
```

:::
:::


### Define our model

Next let's make our model's "recipe". This is the tidymodels term for any pre-processing steps that happen to our data before adding them to a model. A recipe includes our model formula, and any transformations or standardisations we might wish to do[^trans].

[^trans]: For example, log transformation, centring scales, setting dummy variables

In our case, let's define that our model's outcome variable is the `class` of presence or absence. We'll then add our predictor variables to our model, with the formula `class ~ .`, equivalent to `class ~ bio4 + bio12`.

```{r}
#| column: page-inset-right
kookaburras_recipe <- recipe(
  kookaburras_train, 
  formula = class ~ .
  )
kookaburras_recipe
```


```{r}
#| echo: false
#| eval: false
# see what data looks like
# this is more relevant for when lots of steps are added
prep(kookaburras_recipe) |>
  juice()
```

Now we can set our workflow, which merges our formula, any data pre-processing, and specifies which models we'll use. One of the strengths of using tidymodels is that we can run several different types of models in a single [`workflow_set()`](https://workflowsets.tidymodels.org/index.html) to train and optimise them.

```{r}
kookaburras_models <-
  # create the workflow_set
  workflow_set(
    preproc = list(default = kookaburras_recipe),
    models = list(
      glm = sdm_spec_glm(),        # the standard glm specs
      rf = sdm_spec_rf(),          # rf specs with tuning
      gbm = sdm_spec_boost_tree(), # boosted tree model (gbm) specs with tuning
      maxent = sdm_spec_maxent()   # maxent specs with tuning
    ),
    cross = TRUE # make all combinations of preproc and models
  ) |>
  # tweak controls to store information needed later to create the ensemble
  option_add(control = control_ensemble_grid())

kookaburras_models
```

### Fit our model

Next we will determine what parameters optimise our model's performance by **tuning** our model. Tuning uses trial-and-error to figure out which type of model under what hyperparameters makes reasonable predictions.

Let's tune our models using our resampled folds (`kookaburras_cv`).

::: {.callout-note collapse="true"}
## How does tuning work?

Tuning is the process of simulating many different ways to fit lines made by our models to our training data. The number of curves in a model's line of best fit increases as a model becomes more complex, determined by its degrees of freedom. By using different functions to set reasonable weighting parameters that penalize a model when lines curve too much, tuning optimises the model's performance by finding the balance between fitting our current training data and predicting new values correctly. For more information and a visual of this, check out the [tunes package Getting Started vignette](https://tune.tidymodels.org/articles/tune.html).

:::

```{r run-the-model}
#| warning: false
#| message: false
set.seed(2345678) # for reproducability

kookaburras_models_tune <-
  kookaburras_models |>
  workflow_map("tune_grid",
    resamples = kookaburras_cv, 
    grid = 6,                   # increase for more iterations
    metrics = sdm_metric_set(),
    verbose = TRUE,
    control = stacks::control_stack_grid()
  )

kookaburras_models_tune
beepr::beep(2)
```

We can use `autoplot()` to visualise which models performed best by a set of common performance metrics for species distribution models. Models with higher values and smaller confidence intervals performed better.

```{r}
#| fig-align: center
#| fig-column: page
#| fig-width: 8
#| fig-height: 5
#| out-extra: "style=margin-left:auto;margin-right:auto;"
autoplot(kookaburras_models_tune)
```


We can also collect each model's metrics and rank the models by performance.

```{r}
# see metrics
collect_metrics(kookaburras_models_tune) # <1>
```

1.  As a general tidymodels tip, many columns with a `.` at the start of its column name can be retrieved with a `collect_` function (e.g., `collect_metrics()`, `collect_parameters()`).

Our tuning results show that several types of models and parameters performed quite well. Rather than choosing only one model to use for predictions, it's possible to use several as a "stacked ensemble model"! The [stacks package](https://stacks.tidymodels.org/index.html) in tidymodels let's us blend predictions of a few good candidate models (based on whatever metric you choose) to make better overall estimates[^stacks].

[^stacks]: To learn more about how putting together a stack works, check out [this helpful article on the stacks website](https://stacks.tidymodels.org/articles/basics.html#putting-together-a-stack).

```{r}
#| warning: false
library(stacks)
set.seed(123456)

kookaburras_stacked <- 
  stacks() |>                                # initialize the stack
  add_candidates(kookaburras_models_tune) |> # add candidate members
  blend_predictions() |>                     # determine how to combine their predictions
  fit_members()                              # fit the candidates with nonzero stacking coefficients

kookaburras_stacked
```

Here is a nice visual of how these two member models are weighted to inform our predictions.

```{r}
autoplot(kookaburras_stacked, type = "weights")
```


# Assess model

Our model `kookaburras_stacked` is now ready, and we can use it to make predictions about our test data. We'll bind the predicted values to the true values of our test data...

```{r}
kookaburras_test_predictions <-
  kookaburras_test %>%
  bind_cols(predict(kookaburras_stacked, ., 
                    type = "prob", 
                    save_pred = TRUE))
```

...which allows us to assess how good our model is at making correct predictions of the "true" classes.

```{r}
kookaburras_test_predictions |> 
  sdm_metric_set()(truth = class, .pred_presence)
```

We can also visualise how well the model has correctly predicted the class of each point by predicting `"class"` rather than `"prob"` and mapping correct vs incorrect predictions.

```{r}
#| code-fold: true
# predict class
kookaburras_test_predictions_class <-
  kookaburras_test %>%
  bind_cols(predict(kookaburras_stacked, ., 
                    type = "class", 
                    save_pred = TRUE))

# plot correct vs incorrect predictions
kookaburras_test_predictions_class |>
  mutate(correct = case_when(
    class == .pred_class ~ "Correct",
    TRUE ~ "Incorrect"
  )) |>
  ggplot() +
  geom_sf(aes(geometry = geometry, colour = correct)) +
  labs(color = NULL) +
  scale_color_manual(values = c("darkred", "lightpink")) + 
  geom_spatraster(data = aus,
                  aes(fill = wc2.1_30s_bio_4),
                  alpha = 0.1) +
  scale_fill_whitebox_c(palette = "muted",
                        na.value = NA) +
  theme_void()
```

::: {.callout-tip collapse="true"}
## tidysdm wrapper functions

tidysdm offers its own wrapper function [`simple_ensemble()`](https://evolecolgroup.github.io/tidysdm/reference/simple_ensemble.html) to run a stack model workflow and some [helpful ways](https://evolecolgroup.github.io/tidysdm/articles/a2_tidymodels_additions.html#exploring-models-with-dalex) to assess their performance.
:::

### Final prediction

Finally, we can use our model to predict the habitat suitability of laughing kookaburras over our area. We'll predict an entire surface of values within our `aus_filtered` area using the incredible `predict_raster()` function from tidysdm (which saves us quite a few wrangling steps to work nicely with terra).

```{r}
#| fig-width: 9
#| fig-height: 9
#| fig-align: center
#| fig-column: page
#| out-extra: "style=margin-left:auto;margin-right:auto;"
#| lightbox: 
#|   group: final-plot
#|   description: Predicted distribution of laughing kookaburras
# predict
prediction_present <- predict_raster(kookaburras_stacked, 
                                     aus_filtered, 
                                     type = "prob")

# map
ggplot() +
  geom_spatraster(data = prediction_present, 
                  aes(fill = .pred_presence)) +
  scale_fill_whitebox_c(palette = "purple",
                        na.value = NA) +
  guides(
    fill = guide_colorbar(title="Relative\nHabitat\nSuitability")
    ) +

  # plot presences used in the model
  geom_sf(data = kookaburras_sf,
          alpha = 0.3) +
  labs(title="Predicted distribution of laughing kookaburras") +
  pilot::theme_pilot(grid="hv") +
  theme(
    legend.text = element_text(hjust = 0.5)
  )
```

And there we have our predictions of our species distribution model!

# Final thoughts

We hope this article has made the steps of species distribution modelling and interpretation clearer. Species distribution models remain one of the most powerful statistical tools for making inferences about species and their habitat range. tidymodels, tidysdm, and tidyterra offer a useful toolset for running these models in R.

Although our model performed decently under several model performance metrics, no model is perfect. For example, you can see that many of the values towards the centre of Australia have low relative habitat suitability despite quite a few kookaburra occurrences. This is a limitation likely caused by our data and our choice of predictor variables. Testing different subsets of predictors and trying environmental layers at different levels of spatial resolution will help to improve the performance of the model. Another option (if collecting more data is not feasible) is to explore other datasets that could be aggregated to enhance the quality of training data.

Our model above is quite minimalist (for simplicity). If you'd like an example list of variables used in more performant models, check out [this paper](https://academic.oup.com/gigascience/article/doi/10.1093/gigascience/giae002/7619364?login=true#:~:text=prior%20to%20modeling.-,Table%C2%A01%3A,-Summary%20of%20the).

To learn more on ALA Labs, check out our posts on [spatial bias](https://labs.ala.org.au/posts/2022-07-22_sample-bias/) and [mapping multiple overlapping species distributions](https://labs.ala.org.au/posts/2024-01-25_hex_point_maps/).

<details>

<summary style="color: #E06E53;">

Expand for session info

</summary>

```{r, echo = FALSE}
library(sessioninfo)
# save the session info as an object
pkg_sesh <- session_info(pkgs = "attached")
# print it out
pkg_sesh
```

</details>

```{r single-model}
#| eval: false
#| echo: false
## Extracting the result from one model and plotting its accuracy

# choose best model
best_boyce <- kookaburras_models_tune |> 
  extract_workflow_set_result("default_maxent") |>
  select_best("boyce_cont")
best_boyce

# finalize best single model workflow
final_model <- finalize_model(
  sdm_spec_maxent(tune = "sdm"),
  best_boyce
)

final_model


final_wf <- workflow() %>%
  add_recipe(kookaburras_recipe) %>%
  add_model(final_model)

# Let’s make a final workflow, and then fit one last time, using the convenience function last_fit(). 
# This function fits a final model on the entire training set and evaluates on the testing set. 
# We just need to give this funtion our original train/test split.
final_res <- final_wf %>%
  last_fit(kookaburras_split)

final_res

final_res %>%
  collect_metrics()

final_res %>%
  collect_predictions() %>%
  roc_curve(class, .pred_presence) %>%
  autoplot()

final_res %>%
  collect_predictions() %>%
  mutate(correct = case_when(
    class == .pred_class ~ "Correct",
    TRUE ~ "Incorrect"
  )) %>%
  bind_cols(kookaburras_test) %>%
  ggplot() +
  geom_sf(aes(geometry = geometry, colour = correct)) +
  labs(color = NULL) +
  scale_color_manual(values = c("darkred", "lightpink")) + 
  geom_spatraster(data = aus,
                  aes(fill = wc2.1_30s_bio_4),
                  alpha = 0.1) +
  geom_rect(data = custom_bbox,
            mapping = aes(xmin = xmin, 
                          ymin = ymin, 
                          xmax = xmax, 
                          ymax = ymax),
            colour = "grey50",
            fill = NA) +  
  scale_fill_whitebox_c(palette = "muted",
                        na.value = NA) +
  theme_void()

```

```{r}
#| eval: false
#| echo: false
# terra/base equivalent to tidyterra

# plot our new australia layer with Temperature values
plot(aus[[1]])

# plot the points
points(kookaburras[, c("decimalLongitude", "decimalLatitude")], 
       col = "#312108", 
       pch = 16)


# It works, but many users may find it clunkier that ggplot2 to refine. {tidyterra} solves that problem!
```

```{r}
#| eval: false
#| echo: false
# make predictions over multiple layers
prediction_present <- predict_raster(kookaburras_ensemble,
                                     c(aus_filtered,
                                       pop_density_cropped))
```
